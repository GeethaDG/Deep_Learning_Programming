{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment5_V3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jzjPP7GMyR0"
      },
      "source": [
        "# **Assignment 5: Text Classification with RNNs (Part 1)**\n",
        "By Joy Rakshit(231681), Geetha Doddapaneni Gopinath (229498), Sri Chandana Hudukula Ram Kumar (231616)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8k5NN6onhVX",
        "outputId": "a7642aec-2b19-4e5d-ae33-60d28406d63f"
      },
      "source": [
        "from google.colab import drive; drive.mount('/content/drive')\n",
        "import os\n",
        "\n",
        "os.chdir(\"/content/drive/My Drive/Colab Notebooks/IDL/Assignment5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4B_eC6fUhUf"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohAxFS77n4eP"
      },
      "source": [
        "logdir = os.path.join(\"logs\")\n",
        "train_writer = tf.summary.create_file_writer(os.path.join(logdir, \"train\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgLweSGrVHU8"
      },
      "source": [
        "num_words = 20000\n",
        "(train_sequences, train_labels), (test_sequences, test_labels) = tf.keras.datasets.imdb.load_data(num_words=num_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LopPWAghZCOK"
      },
      "source": [
        "word_to_index = tf.keras.datasets.imdb.get_word_index()\n",
        "index_to_word = dict((index, word) for (word, index) in word_to_index.items())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "4C4Yf_3GVSy2",
        "outputId": "bdf5f55e-8ca1-4b96-c67d-2fb169d877e5"
      },
      "source": [
        "test_sequences[0]\n",
        "index_to_word[1035]\n",
        "\n",
        "\" \".join([index_to_word[index ] for index in test_sequences[0][1:]])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"wonder own as by is sequence i i jars roses to of hollywood br of down shouting getting boring of ever it sadly sadly sadly i i was then does don't close faint after one carry as by are be favourites all family turn in does as three part in another some to be probably with world uncaring her an have faint beginning own as is sequence\""
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 264
        },
        "id": "aUg8qX9JVQZd",
        "outputId": "74cb7fa8-852a-4ed6-d379-4e3980fbb4ca"
      },
      "source": [
        "sequence_lengths = [len(sequence) for sequence in train_sequences]\n",
        "max_len = 200\n",
        "max_len\n",
        "plt.hist(sequence_lengths, bins=80)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "_odsI5k6ZT6D",
        "outputId": "741b10be-c5b6-434d-93e9-6d2231b7a156"
      },
      "source": [
        "sequence_lengths = [len(sequence) for sequence in train_sequences]\n",
        "unique,counts = np.unique(sequence_lengths, return_counts=True)\n",
        "data = dict(zip(unique,counts))\n",
        "plt.bar(range(len(data)), counts,align='edge')\n",
        "plt.xticks(rotation='vertical')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([-200.,    0.,  200.,  400.,  600.,  800., 1000., 1200.]),\n",
              " <a list of 8 Text major ticklabel objects>)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 316
        },
        "id": "U6K_VYpksz_y",
        "outputId": "36875d72-e555-4b19-bd09-41952747cf31"
      },
      "source": [
        "sequence_lengths = [len(sequence) for sequence in test_sequences]\n",
        "unique,counts = np.unique(sequence_lengths, return_counts=True)\n",
        "data = dict(zip(unique,counts))\n",
        "plt.bar(range(len(data)), counts,align='edge')\n",
        "plt.xticks(rotation='vertical')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([-200.,    0.,  200.,  400.,  600.,  800., 1000., 1200.]),\n",
              " <a list of 8 Text major ticklabel objects>)"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEHCAYAAABV4gY/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAT7klEQVR4nO3dfbBkdX3n8fcHJloJKSPIzSwRyKBBU2RLR/cWWmVM4QNxMCmfKjFMJYqsm9FaqZhyaxPUqpiHyhb7QFxTZnXHQMAqM2okllQgCjG7Wu4G8UJGAsqjgTgTmLlKJE+WFeCbP/pcaS73zr23H253//r9quq63b/T5/S3OcPn/Pp3fqc7VYUkqS3HTboASdLoGe6S1CDDXZIaZLhLUoMMd0lq0I5JFwBw8skn165duyZdhiTNlJtuuukbVbWw1rKpCPddu3axtLQ06TIkaaYkuW+9ZQ7LSFKDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwz3Edp18TWTLkGSAMN9LAx5SZNmuEtSgwz3EbPXLmkaGO6S1CDDXZIaZLhLUoM2DPcklyc5muTWvraPJTnY3e5NcrBr35Xk233LPjjO4iVJa9vMLzFdAbwf+PBKQ1X93Mr9JJcCD/U9/56q2j2qAiVJW7dhuFfV55PsWmtZkgCvB1462rIkScMYdsz9xcCRqrqrr+2MJH+Z5HNJXrzeikn2JVlKsrS8vDxkGZKkfsOG+17gQN/j+4HTq+p5wDuAP0zylLVWrKr9VbVYVYsLC2v+eLckaUADh3uSHcDrgI+ttFXVd6rqm939m4B7gGcNW6QkaWuG6bm/HLi9qg6tNCRZSHJ8d/8ZwJnA14YrUZK0VZuZCnkA+Avg2UkOJXlzt+h8Hj8kA/ATwC3d1MhPAG+tqgdHWbAkaWObmS2zd532N63RdhVw1fBlSZKG4RWqktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDfRvsuviaSZcgac4Y7pLUoM38QPblSY4mubWv7deTHE5ysLu9sm/ZO5PcneSOJK8YV+GSpPVtpud+BbBnjfb3VtXu7nYtQJKzgPOBH+vW+V9Jjh9VsZKkzdkw3Kvq88CDm9zeq4GPVtV3quqvgbuBs4eoT5I0gGHG3C9Kcks3bHNi1/Z04Ot9zznUtT1Bkn1JlpIsLS8vD1GGJGm1QcP9A8Azgd3A/cClW91AVe2vqsWqWlxYWBiwDEnSWgYK96o6UlWPVNWjwId4bOjlMHBa31NP7dokSdtooHBPckrfw9cCKzNprgbOT/LkJGcAZwI3DleiJGmrdmz0hCQHgHOAk5McAt4DnJNkN1DAvcBbAKrqtiQfB74CPAy8raoeGU/pkqT1bBjuVbV3jebLjvH83wZ+e5iiJEnD8QpVSWqQ4S5JDTLcx8gvDJM0KYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHAfAa9ElTRtDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUoA3DPcnlSY4mubWv7b8nuT3JLUk+meSpXfuuJN9OcrC7fXCcxc+ClWmSTpeUtJ0203O/Atizqu164N9W1XOAO4F39i27p6p2d7e3jqZMSdJWbBjuVfV54MFVbddV1cPdwxuAU8dQmyRpQKMYc//3wJ/2PT4jyV8m+VySF6+3UpJ9SZaSLC0vL4+gDEnSiqHCPcm7gYeBj3RN9wOnV9XzgHcAf5jkKWutW1X7q2qxqhYXFhaGKUOStMrA4Z7kTcBPAz9fVQVQVd+pqm92928C7gGeNYI6m+LJVUnjNlC4J9kD/Arwqqr65772hSTHd/efAZwJfG0UhU4rg1rSNNqx0ROSHADOAU5Ocgh4D73ZMU8Grk8CcEM3M+YngN9M8i/Ao8Bbq+rBNTcsSRqbDcO9qvau0XzZOs+9Crhq2KIkScPxClVJapDhLkkNMtwlqUGG+zZzdo2k7WC4S1KDDHdJapDhLkkNMtyH4Pi5pGlluEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDfcKcKy9pHAx3SWqQ4T4F7L1LGjXDXZIatKlwT3J5kqNJbu1rOynJ9Unu6v6e2LUnye8muTvJLUmeP67iW2ZvXtIwNttzvwLYs6rtYuCzVXUm8NnuMcB5wJndbR/wgeHLlCRtxabCvao+Dzy4qvnVwJXd/SuB1/S1f7h6bgCemuSUURQrSdqcYcbcd1bV/d39B4Cd3f2nA1/ve96hru1xkuxLspRkaXl5eYgyJEmrjeSEalUVUFtcZ39VLVbV4sLCwijKkCR1hgn3IyvDLd3fo137YeC0vued2rWpjydMJY3TMOF+NXBBd/8C4FN97W/sZs28EHiob/hGkrQNdmzmSUkOAOcAJyc5BLwHuAT4eJI3A/cBr++efi3wSuBu4J+BC0dcsyRpA5sK96rau86il63x3ALeNkxRkqTheIXqlHAMXtIoGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJ8yzneXNAqGuyQ1yHCfEfboJW2F4S5JDTLcJalBhvsUWj0E45CMpK0y3CWpQYa7JDXIcJekBhnuktQgw31GeZJV0rFs6jdU15Lk2cDH+pqeAfwa8FTgF4Hlrv1dVXXtwBVKkrZs4HCvqjuA3QBJjgcOA58ELgTeW1X/YyQVSpK2bFTDMi8D7qmq+0a0PUnSEEYV7ucDB/oeX5TkliSXJzlxrRWS7EuylGRpeXl5radIkgY0dLgneRLwKuCPuqYPAM+kN2RzP3DpWutV1f6qWqyqxYWFhWHLmAsrJ1E9mSppI6PouZ8H3FxVRwCq6khVPVJVjwIfAs4ewWtIkrZgFOG+l74hmSSn9C17LXDrCF5DkrQFA8+WAUhyAnAu8Ja+5v+WZDdQwL2rlmmLHIKRNIihwr2q/gl42qq2NwxVkSRpaF6hKkkNMtwlqUGGe4Mcp5dkuEtSgwx3SWqQ4S5JDTLcG+bYuzS/DPcBGJqSpp3hvkXTGuzTWpekyTDcJalBhrskNWjuw93hDEktmvtwl6QWGe6S1CDDXZIaZLjPMM8XSFqP4b4FhqmkWWG4N8CDjqTVDHdJatDQ4Z7k3iR/leRgkqWu7aQk1ye5q/t74vCljpa9XUktG1XP/SVVtbuqFrvHFwOfraozgc92jzVm/QcsD17SfBvXsMyrgSu7+1cCrxnT64ydISlpFo0i3Au4LslNSfZ1bTur6v7u/gPAztUrJdmXZCnJ0vLy8gjKGA17v5JasGME2/jxqjqc5AeB65Pc3r+wqipJrV6pqvYD+wEWFxefsFySNLihe+5Vdbj7exT4JHA2cCTJKQDd36PDvo4kafOG6rknOQE4rqr+obv/k8BvAlcDFwCXdH8/NWyh4+YQjKSWDNtz3wl8IcmXgRuBa6rq0/RC/dwkdwEv7x5P3OoAP1agG/aSZtlQPfeq+hrw3DXavwm8bJhtS5IG5xWqktSguQn3eR9mmff3L82buQn3eebcfWn+GO6S1CDDXZIaZLhLUoPmLtwdc5Y0D+Yu3LW1A5wHQ2k2Ge6b0GrAbfS+1lre6n8LqTWGuyQ1yHDXd02iV+4nAWk8DHetydCVZpvhPucMcalNhrskNchwn1PD9Nj9Hnxp+hnuehyDW2qD4T5HDGdpfhjuAsYb/Ott24ONND6GuyQ1yHDXyNgTl6bHwOGe5LQk/yfJV5LcluTtXfuvJzmc5GB3e+XoypUkbcYwPfeHgf9UVWcBLwTeluSsbtl7q2p3d7t26Co1cat75cf66b7N9ODt5UvjtWPQFavqfuD+7v4/JPkq8PRRFSZJGtxIxtyT7AKeB3yxa7ooyS1JLk9y4jrr7EuylGRpeXl5FGVIkjpDh3uS7weuAn65qv4e+ADwTGA3vZ79pWutV1X7q2qxqhYXFhaGLUMT4vCKNJ2GCvck30Mv2D9SVX8MUFVHquqRqnoU+BBw9vBlSpK2YpjZMgEuA75aVb/T135K39NeC9w6eHmaZoP22u3tS+M38AlV4EXAG4C/SnKwa3sXsDfJbqCAe4G3DFWhJGnLhpkt8wUgayxy6qO+61hTJiWNj1eoauJWHwA8CEjDM9w1FUYR6B4UpMcY7pLUIMNdU2nUXxNsr17zxnDX2IwyUKd1W9K0aj7c/R959m20D93H0hM1H+6abWsFt2EubcxwVxMG+drhUb/mqJ8vDcNwl6QGGe6aSSu94GMN24z6JKw9b80Sw11i/YOEga5ZZbhLUoOG+VbIqWaPS6vtuvga7r3kp757f8VK21a3JU0ze+6aWaMK2GG3s94Y/2ancU5ipo/aZ7ireZsNy0keLI61znaFvQeVtjQZ7v4j1bQY1aeC1W3H+u6djU4E+0lhPjQZ7tKgBp1GuZmpmes93qh9kG1t9TmbdawfX9nsgWjUXwq3FfN0xbPhLm3BKHu94x6K2Wj7w/xK1qiuJZjmIaf19vUkD05b0VS4T9t/XGk7jGM+/maGd9Zbb9jXXevxJH+ucbs/HY3K2MI9yZ4kdyS5O8nF43qdFdP4H1faqnH1Crd7/WGGY1YH+mauQj7Wa613gNjo8WZq30pd251RYwn3JMcDvwecB5wF7E1y1jheS5L0ROO6iOls4O6q+hpAko8Crwa+Mo4Xs9euWbWd/3b7L+La6Hlbad9o2XrPG9WU0UFO8m71NVYv28xFcBt9etho/WGlqka/0eRngD1V9R+6x28AXlBVF/U9Zx+wr3v4bOCOIV7yZOAbQ6w/i+bxPYPve974vo/th6tqYa0FE/v6garaD+wfxbaSLFXV4ii2NSvm8T2D73vSdWw33/fgxnVC9TBwWt/jU7s2SdI2GFe4fwk4M8kZSZ4EnA9cPabXkiStMpZhmap6OMlFwGeA44HLq+q2cbxWZyTDOzNmHt8z+L7nje97QGM5oSpJmqymrlCVJPUY7pLUIMNdkho0cz+zl+RH6V3t+vSu6TBwdVV9dXJVadySnARQVQ9OuhaNl/t6NGaq557kV4GPAgFu7G4BDmzHl5NNWpKdSZ7f3XZOup5xS3J6ko8mWQa+CNyY5GjXtmuy1Y3fPO3ved/X4zBTs2WS3An8WFX9y6r2JwG3VdWZk6lsvJLsBj4I/ACPXQx2KvAt4D9W1c2Tqm2ckvwF8D+BT1TVI13b8cDPAr9cVS+cZH3jMo/7e1739YokPwDs4fEjEp+pqm8NvM0ZC/fbgVdU1X2r2n8YuK6qnj2ZysYryUHgLVX1xVXtLwT+d1U9dzKVjVeSu9Y7YB9r2aybx/09r/saIMkbgfcA1/H4g/m5wG9U1YcH2u6Mhfse4P3AXcDXu+bTgR8BLqqqT0+qtnHa4B/+3VX1I9td03bovk30QeBKHtvfpwEXACdX1esnVds4zeP+ntd9DZDkDnpfrPitVe0nAl+sqmcNst2ZOqFaVZ9O8ix6Xync//HlSysf5Rr1p0muAT7M4//hvxFo8oDWeSPwZuA3WHUCHbhsUkVtg3nc3/O6r6F33nCtXvaj3bLBNjpLPfd5luQ81p4ldO3kqtK4uL/nR5ILgF+jNyzTPyJxLvBbVXXFQNs13DWtkuyg15t7DY8PuU8Bl60+sa7ZNe/7uhuCeQVPPKH6dwNv03Cfft2Z9HfS68ntpPcR7ii9f/iXDHNGfZolOUBvhsiVwKGu+VR647AnVdXPTaq2cZrH/T2v+7pfN931u+FeVUeG2p7hPv2SfAb4c+DKqnqga/s3wJuAl1bVT06wvLFJcud6J5OOtWzWzeP+ntd9DU+Y+nqI3jj70FNfDfcZkOSO9aZ5HmvZrEtyA3ApcFVVPdq1HUdv7vM7quoFk6xvXOZxf8/rvobxTX2dqStU59h9SX6l/yrF7urFX+WxEzAtOh/4GeBIkjuT3AU8ALyuW9aqedzfK/v6gW5f38l87GuAE1YHO0BV3QCcMOhG7bnPgO5ky8U8fgz2CL1pYv91Hr6DI8nTurvvq6pfmGgxYzaP+7u7ynwv8LfAzfSu1nwRcBuwv+UTqkl+F3gma099/euqumig7Rrus6H7wrRTgRuq6h/72vc0fPHWWj/N+FJ649FU1au2t6LtkeQFwO1V9VCS76MX9M+nF3T/paoemmiBY5DkI/Suu/le4CF6PdZPAi+jl1MXTLC8sRvH1FfDfQYk+SXgbcBXgd3A26vqU92ym6vq+ZOsb1yS3Ax8Bfh9er3XAAfoPqZX1ecmV934JLkNeG73c5X7gX8CrqIXdM+tqtdNtMAxSHJLVT2nmxJ5GPihqnokSYAvV9VzJlzizJmpK1Tn2C8C/66q/rH7hrxPJNlVVe9jiCvYZsAi8Hbg3cB/rqqDSb7daqj3Oa6qHu7uL/YdvL/QnXxr0XHd0MwJwPfRmznyIPBk4HsmWdi4jWvqqydUZ8NxK0MxVXUvcA5wXpLfoeFwr6pHq+q9wIXAu5O8n/nokNya5MLu/peTLAJ0X73R6tjzZcDtwEF6B/M/SvIh4Ev0vua7ZR8H/g54SVWdVFVPA15CbyrkxwfdqMMyMyDJn9ObDnawr20HcDnw81V1/MSK20ZJfgp4UVW9a9K1jFPXk3sf8GLgG/TG27/e3X6pqr48wfLGJskPAVTV3yZ5KvBy4G+q6sbJVjZe45r6arjPgCSnAg+vXNCyatmLqur/TaAsjVmSpwBn0Pu0cmjYKxY1nZJcB/wZvYvWjnRtO+ldtHZuVb18oO0a7pI0Oaumvv5g17wy9fWSQb9fxnCXpCmV5MKq+oOB1jXcJWk6Jfmbqjp9kHXnYeaBJE2tJLest4je1MiBGO6SNFk76X2X++qx9QD/f9CNGu6SNFl/Anx//1TnFUn+76AbdcxdkhrkFaqS1CDDXZIaZLhLUoMMd0lq0L8CBnfhgH3fregAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iArlNrPwVQYN"
      },
      "source": [
        "maxlen = 200\n",
        "\n",
        "train_sequences_padded = tf.keras.preprocessing.sequence.pad_sequences(train_sequences, padding='post', maxlen=maxlen)\n",
        "train_data = tf.data.Dataset.from_tensor_slices((train_sequences_padded, train_labels))\n",
        "\n",
        "test_sequences_padded = tf.keras.preprocessing.sequence.pad_sequences(test_sequences, padding='post', maxlen=maxlen)\n",
        "test_data = tf.data.Dataset.from_tensor_slices((test_sequences_padded, test_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e5Yt8MnoVQWz",
        "outputId": "f5d5dd06-4bdb-43df-b55b-9e7ab7c313a6"
      },
      "source": [
        "train_sequences_padded.shape\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(25000, 200)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8o9gjo_LVQU-"
      },
      "source": [
        "def train_loop():\n",
        "    step = 1\n",
        "    sum_loss = 0\n",
        "    acc = 0\n",
        "\n",
        "    for sequence_batch, label_batch in train_data:\n",
        "        logits,loss = train_step(sequence_batch, label_batch, step)\n",
        "\n",
        "        preds = tf.argmax(logits, axis=1, output_type=tf.int64)\n",
        "        acc += tf.reduce_mean(tf.cast(tf.equal(preds, tf.reshape(label_batch,[-1])),\n",
        "                             tf.float32))\n",
        "        sum_loss = sum_loss + loss\n",
        "        if not step % 2500:\n",
        "          print(\"Av_Loss: {} Av_Accuracy: {}\".format(sum_loss/step, acc/step))\n",
        "\n",
        "        step += 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3rb3IJP6PEu"
      },
      "source": [
        "# a single training step\n",
        "def train_step(sequences, labels, step):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = rnn_loop(sequences)  \n",
        "        loss = loss_fn(tf.reshape(labels,[-1]), logits)\n",
        "\n",
        "    gradient = tape.gradient(loss, [u,w,b,v,c,start_var])\n",
        "    u.assign_sub(learning_rate * gradient[0])\n",
        "    w.assign_sub(learning_rate * gradient[1])\n",
        "    b.assign_sub(learning_rate * gradient[2])\n",
        "    v.assign_sub(learning_rate * gradient[3])\n",
        "    c.assign_sub(learning_rate * gradient[4])\n",
        "    start_var.assign_sub(learning_rate * gradient[5])\n",
        "\n",
        "    with train_writer.as_default():\n",
        "          tf.summary.scalar(\"loss\", loss, step=step)\n",
        "          tf.summary.histogram(\"logits\", logits, step=step)\n",
        "          tf.summary.histogram(\"weights_u\", u, step=step)\n",
        "          tf.summary.histogram(\"weights_w\", w, step=step)\n",
        "          tf.summary.histogram(\"weights_v\", v, step=step)\n",
        "          tf.summary.histogram(\"weights_start_var\", start_var, step=step)\n",
        "    return logits, loss\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7bKrH2X02wD"
      },
      "source": [
        "def rnn_loop(sequences):\n",
        "    old_state = start_var # initial state started with zero tensors\n",
        "    for step in range(max_len):\n",
        "\n",
        "        x_t = sequences[step]\n",
        "        x_t = tf.one_hot(x_t, depth=num_words) #\n",
        "        #tf.print(x_t)\n",
        "        new_state = rnn_step(old_state, x_t)\n",
        "\n",
        "        old_state = new_state\n",
        "\n",
        "    o_t = output_layer(new_state)\n",
        "\n",
        "    return o_t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNGUSgbl7eJi"
      },
      "source": [
        "#Formula implementation from pg.370 pg.371\n",
        "def rnn_step(state, x_t):\n",
        "    a_t = b + tf.matmul(state,w) + tf.matmul(tf.reshape(x_t,[-1,num_words]),u)\n",
        "    h_t = tf.tanh(a_t)\n",
        "\n",
        "    return h_t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHqwCeqwVqUu"
      },
      "source": [
        "def output_layer(new_state):\n",
        "  o_t = c + tf.matmul(new_state,v)\n",
        "\n",
        "  return o_t"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wovTlJmZr5Yq"
      },
      "source": [
        "def loss_fn(labels, logits):\n",
        "  loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
        "            logits=logits, labels=labels))\n",
        "  \n",
        "  return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aS4i2dntKUpW"
      },
      "source": [
        "# **Experiment 1 :**\n",
        "- Softmax\n",
        "- One Hidden Layer \n",
        "- Learning Rate = 0.1\n",
        "- Maximum Length = 200"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rk_Zwhej196a",
        "outputId": "2e03050d-2ba1-4cb7-ac1d-e05605edc119"
      },
      "source": [
        "hidd_unit = 1\n",
        "final_unit = 2\n",
        "\n",
        "\n",
        "learning_rate = 0.1\n",
        "\n",
        "u = tf.Variable(tf.random.uniform(shape=[num_words,hidd_unit],minval=-0.1,maxval=0.1,dtype=tf.dtypes.float32))   \n",
        "w = tf.Variable(tf.random.uniform(shape=[hidd_unit,hidd_unit],minval=-0.1,maxval=0.1,dtype=tf.dtypes.float32))\n",
        "b = tf.Variable(tf.random.uniform(shape=[hidd_unit],minval=-0.1,maxval=0.1,dtype=tf.dtypes.float32))\n",
        "v = tf.Variable(tf.random.uniform(shape=[hidd_unit,final_unit],minval=-0.1,maxval=0.1,dtype=tf.dtypes.float32))\n",
        "c = tf.Variable(tf.random.uniform(shape=[final_unit],minval=-0.1,maxval=0.1,dtype=tf.dtypes.float32))\n",
        "\n",
        "start_var = tf.Variable(tf.zeros([hidd_unit,hidd_unit])) # initial state started with zero tensors\n",
        "\n",
        "train_loop()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Av_Loss: 0.7047434449195862 Av_Accuracy: 0.5023999810218811\n",
            "Av_Loss: 0.7057473063468933 Av_Accuracy: 0.5040000081062317\n",
            "Av_Loss: 0.7070624232292175 Av_Accuracy: 0.4952000081539154\n",
            "Av_Loss: 0.7067931890487671 Av_Accuracy: 0.49889999628067017\n",
            "Av_Loss: 0.7063465714454651 Av_Accuracy: 0.5006399750709534\n",
            "Av_Loss: 0.7063488960266113 Av_Accuracy: 0.49959999322891235\n",
            "Av_Loss: 0.706001877784729 Av_Accuracy: 0.5008000135421753\n",
            "Av_Loss: 0.7059129476547241 Av_Accuracy: 0.5020999908447266\n",
            "Av_Loss: 0.7059192061424255 Av_Accuracy: 0.5019555687904358\n",
            "Av_Loss: 0.705808699131012 Av_Accuracy: 0.5031200051307678\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXHPtU7AKUpZ"
      },
      "source": [
        "# **Experiment 2 :**\n",
        "- Softmax\n",
        "- One Hidden Layer \n",
        "- Learning Rate = 0.1\n",
        "- Maximum Length = 400\n",
        "- Removing records with lenght greater than 400"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1KsjJ-4fsW3_"
      },
      "source": [
        "train_sequences_400 = []\n",
        "train_label_400 = []\n",
        "test_sequences_400 = []\n",
        "test_label_400 = []\n",
        "train_discarded = []\n",
        "train_label_discarded = []\n",
        "test_discarded = []\n",
        "test_label_discarded = []\n",
        "for i in range(len(train_sequences)):\n",
        "  if len(train_sequences[i]) <= 400:\n",
        "    train_sequences_400.append(train_sequences[i])\n",
        "    train_label_400.append(train_labels[i])\n",
        "  else:\n",
        "    train_discarded.append(train_sequences[i])\n",
        "    train_label_discarded.append(train_labels[i])\n",
        "\n",
        "for i in range(len(test_sequences)):\n",
        "  if len(test_sequences[i]) <= 400:\n",
        "    test_sequences_400.append(test_sequences[i])\n",
        "    test_label_400.append(test_labels[i])\n",
        "  else:\n",
        "    test_discarded.append(test_sequences[i])\n",
        "    test_label_discarded.append(test_labels[i])\n",
        "\n",
        "train_label_400 = np.asarray(train_label_400,dtype=np.int64)\n",
        "test_label_400 = np.asarray(test_label_400,dtype=np.int64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1HodhH1uNXC",
        "outputId": "7d100608-681b-461a-943e-9c0ea2963e24"
      },
      "source": [
        "#Class Imbalance Check\n",
        "unique_tr, counts_tr = np.unique(train_labels, return_counts=True)\n",
        "print('Number for records per class in train data before discarding: \\n class 0 :  {} \\n class 1 : {}'.format(counts_tr[0],counts_tr[1]))\n",
        "unique_4tr, counts_4tr = np.unique(train_label_400, return_counts=True)\n",
        "print('Number for records per class in train data after discarding: \\n class 0 :  {} \\n class 1 : {}'.format(counts_4tr[0],counts_4tr[1]))\n",
        "\n",
        "unique_tt, counts_tt = np.unique(test_labels, return_counts=True)\n",
        "print('Number for records per class in test data before discarding: \\n class 0 :  {} \\n class 1 : {}'.format(counts_tt[0],counts_tt[1]))\n",
        "unique_4tt, counts_4tt = np.unique(test_label_400, return_counts=True)\n",
        "print('Number for records per class in test data after discarding: \\n class 0 :  {} \\n class 1 : {}'.format(counts_4tt[0],counts_4tt[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number for records per class in train data before discarding: \n",
            " class 0 :  12500 \n",
            " class 1 : 12500\n",
            "Number for records per class in train data after discarding: \n",
            " class 0 :  10864 \n",
            " class 1 : 10652\n",
            "Number for records per class in test data before discarding: \n",
            " class 0 :  12500 \n",
            " class 1 : 12500\n",
            "Number for records per class in test data after discarding: \n",
            " class 0 :  10957 \n",
            " class 1 : 10859\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IiQlyXdRqmNK"
      },
      "source": [
        "maxlen = 400\n",
        "\n",
        "train_sequences_padded = tf.keras.preprocessing.sequence.pad_sequences(train_sequences_400, padding='post', maxlen=maxlen)\n",
        "train_data = tf.data.Dataset.from_tensor_slices((train_sequences_padded, train_label_400))\n",
        "\n",
        "test_sequences_padded = tf.keras.preprocessing.sequence.pad_sequences(test_sequences_400, padding='post', maxlen=maxlen)\n",
        "test_data = tf.data.Dataset.from_tensor_slices((test_sequences_padded, test_label_400))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHYiGmsXKUpc",
        "outputId": "51676657-e6b0-4b24-c2b1-597f26ba626c"
      },
      "source": [
        "hidd_unit = 1\n",
        "final_unit = 2\n",
        "\n",
        "\n",
        "learning_rate = 0.1\n",
        "\n",
        "u = tf.Variable(tf.random.uniform(shape=[num_words,hidd_unit],minval=-0.1,maxval=0.1,dtype=tf.dtypes.float32))   \n",
        "w = tf.Variable(tf.random.uniform(shape=[hidd_unit,hidd_unit],minval=-0.1,maxval=0.1,dtype=tf.dtypes.float32))\n",
        "b = tf.Variable(tf.random.uniform(shape=[hidd_unit],minval=-0.1,maxval=0.1,dtype=tf.dtypes.float32))\n",
        "v = tf.Variable(tf.random.uniform(shape=[hidd_unit,final_unit],minval=-0.1,maxval=0.1,dtype=tf.dtypes.float32))\n",
        "c = tf.Variable(tf.random.uniform(shape=[final_unit],minval=-0.1,maxval=0.1,dtype=tf.dtypes.float32))\n",
        "\n",
        "start_var = tf.Variable(tf.zeros([hidd_unit,hidd_unit])) # initial state started with zero tensors\n",
        "\n",
        "\n",
        "train_loop()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Av_Loss: 0.7050085663795471 Av_Accuracy: 0.5091999769210815\n",
            "Av_Loss: 0.7074670195579529 Av_Accuracy: 0.49239999055862427\n",
            "Av_Loss: 0.7070003747940063 Av_Accuracy: 0.49693334102630615\n",
            "Av_Loss: 0.7068579196929932 Av_Accuracy: 0.49900001287460327\n",
            "Av_Loss: 0.707098662853241 Av_Accuracy: 0.4960800111293793\n",
            "Av_Loss: 0.7067357897758484 Av_Accuracy: 0.49666666984558105\n",
            "Av_Loss: 0.7064630389213562 Av_Accuracy: 0.49805715680122375\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyMBR0YTKUpd"
      },
      "source": [
        "# **Experiment 3 :**\n",
        "- Softmax\n",
        "- One Hidden Layer \n",
        "- Learning Rate = 0.5\n",
        "- maximum length = 200\n",
        "- pre padding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ilPuwp82KUpd"
      },
      "source": [
        "maxlen = 200\n",
        "\n",
        "train_sequences_padded = tf.keras.preprocessing.sequence.pad_sequences(train_sequences, padding='pre', maxlen=maxlen)\n",
        "train_data = tf.data.Dataset.from_tensor_slices((train_sequences_padded, train_labels))\n",
        "\n",
        "test_sequences_padded = tf.keras.preprocessing.sequence.pad_sequences(test_sequences, padding='pre', maxlen=maxlen)\n",
        "test_data = tf.data.Dataset.from_tensor_slices((test_sequences_padded, test_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MDaG5S0KUpd",
        "outputId": "b69363c9-9647-4191-af38-2abb3862dbd3"
      },
      "source": [
        "hidd_unit = 1\n",
        "final_unit = 2\n",
        "\n",
        "\n",
        "learning_rate = 0.5 #trying with larger learning rate\n",
        "\n",
        "u = tf.Variable(tf.random.uniform(shape=[num_words,hidd_unit],minval=-0.1,maxval=0.1,dtype=tf.dtypes.float32)) \n",
        "w = tf.Variable(tf.random.uniform(shape=[hidd_unit,hidd_unit],minval=-0.1,maxval=0.1,dtype=tf.dtypes.float32))\n",
        "b = tf.Variable(tf.random.uniform(shape=[hidd_unit],minval=-0.1,maxval=0.1,dtype=tf.dtypes.float32))##\n",
        "v = tf.Variable(tf.random.uniform(shape=[hidd_unit,final_unit],minval=-0.1,maxval=0.1,dtype=tf.dtypes.float32))\n",
        "c = tf.Variable(tf.random.uniform(shape=[final_unit],minval=-0.1,maxval=0.1,dtype=tf.dtypes.float32))\n",
        "\n",
        "start_var = tf.Variable(tf.zeros([hidd_unit,hidd_unit])) # initial state started with zero tensors\n",
        "\n",
        "\n",
        "train_loop()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Av_Loss: 0.7757892608642578 Av_Accuracy: 0.5004000067710876\n",
            "Av_Loss: 0.8211752772331238 Av_Accuracy: 0.49959999322891235\n",
            "Av_Loss: 0.8444502353668213 Av_Accuracy: 0.48973333835601807\n",
            "Av_Loss: 0.8489469885826111 Av_Accuracy: 0.49300000071525574\n",
            "Av_Loss: 0.8524800539016724 Av_Accuracy: 0.49511998891830444\n",
            "Av_Loss: 0.855155885219574 Av_Accuracy: 0.4962666630744934\n",
            "Av_Loss: 0.8563905954360962 Av_Accuracy: 0.4969714283943176\n",
            "Av_Loss: 0.8581541180610657 Av_Accuracy: 0.49755001068115234\n",
            "Av_Loss: 0.8585373163223267 Av_Accuracy: 0.4980444312095642\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HctvFslKUpe"
      },
      "source": [
        "# **Experiment 4 :**\n",
        "- Softmax\n",
        "- One Hidden Layer \n",
        "- Learning Rate = 0.1\n",
        "- Variable length dataset using tf.ragged"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "659O7omjKUpe"
      },
      "source": [
        "train_sequences_ragged = tf.ragged.constant(train_sequences)\n",
        "test_sequences_ragged = tf.ragged.constant(test_sequences)\n",
        "\n",
        "#train_sequences_padded = tf.keras.preprocessing.sequence.pad_sequences(train_sequences, padding='post', maxlen=maxlen)\n",
        "train_data = tf.data.Dataset.from_tensor_slices((train_sequences_ragged, train_labels))\n",
        "\n",
        "#test_sequences_padded = tf.keras.preprocessing.sequence.pad_sequences(test_sequences, padding='post', maxlen=maxlen)\n",
        "test_data = tf.data.Dataset.from_tensor_slices((test_sequences_ragged, test_labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwzzD2etKUpf",
        "outputId": "bf5161fb-7861-4194-86a7-badf7e9f0c4c"
      },
      "source": [
        "hidd_unit = 1\n",
        "final_unit = 2\n",
        "\n",
        "\n",
        "learning_rate = 0.1\n",
        "\n",
        "u = tf.Variable(tf.random.uniform(shape=[num_words,hidd_unit],minval=-0.1,maxval=0.1,dtype=tf.dtypes.float32))   \n",
        "w = tf.Variable(tf.random.uniform(shape=[hidd_unit,hidd_unit],minval=-0.1,maxval=0.1,dtype=tf.dtypes.float32))\n",
        "b = tf.Variable(tf.random.uniform(shape=[hidd_unit],minval=-0.1,maxval=0.1,dtype=tf.dtypes.float32))\n",
        "v = tf.Variable(tf.random.uniform(shape=[hidd_unit,final_unit],minval=-0.1,maxval=0.1,dtype=tf.dtypes.float32))\n",
        "c = tf.Variable(tf.random.uniform(shape=[final_unit],minval=-0.1,maxval=0.1,dtype=tf.dtypes.float32))\n",
        "\n",
        "start_var = tf.Variable(tf.zeros([hidd_unit,hidd_unit])) # initial state started with zero tensors\n",
        "\n",
        "start_time = time.time()\n",
        "train_loop()\n",
        "end_time = time.time()\n",
        "print(\"Total Time taken: \",end_time-start_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Av_Loss: 0.7047833800315857 Av_Accuracy: 0.5012000203132629\n",
            "Av_Loss: 0.7056570053100586 Av_Accuracy: 0.5019999742507935\n",
            "Av_Loss: 0.7063391208648682 Av_Accuracy: 0.5001333355903625\n",
            "Av_Loss: 0.7052043080329895 Av_Accuracy: 0.5072000026702881\n",
            "Av_Loss: 0.7037671804428101 Av_Accuracy: 0.5119199752807617\n",
            "Av_Loss: 0.702365517616272 Av_Accuracy: 0.5155333280563354\n",
            "Av_Loss: 0.7011614441871643 Av_Accuracy: 0.5216571688652039\n",
            "Av_Loss: 0.7004018425941467 Av_Accuracy: 0.5241000056266785\n",
            "Av_Loss: 0.6996601819992065 Av_Accuracy: 0.526711106300354\n",
            "Av_Loss: 0.7015451788902283 Av_Accuracy: 0.5250800251960754\n",
            "Total Time taken:  5904.213921785355\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mDoiXnGKUpf"
      },
      "source": [
        "# **Experiment** 5 :\n",
        "- Binary cross entrophy\n",
        "- One Hidden Layer \n",
        "- Learning Rate = 0.1\n",
        "- Variable length dataset using tf.ragged"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ww2Kh54UKUpg"
      },
      "source": [
        "def train_loop():\n",
        "    step = 1\n",
        "    sum_loss = 0\n",
        "    acc = 0\n",
        "\n",
        "    for sequence_batch, label_batch in train_data:\n",
        "        logits,loss = train_step(sequence_batch, label_batch, step)\n",
        "\n",
        "        preds = tf.argmax(logits, axis=1, output_type=tf.int64)\n",
        "        acc += tf.reduce_mean(tf.cast(tf.equal(preds, tf.reshape(label_batch,[-1])),\n",
        "                             tf.float32))\n",
        "        sum_loss = sum_loss + loss\n",
        "        if not step % 2500:\n",
        "          print(\"Av_Loss: {} Av_Accuracy: {}\".format(sum_loss/step, acc/step))\n",
        "\n",
        "        step += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ac76h5nGKUpg"
      },
      "source": [
        "def loss_fn(labels, logits):\n",
        "    binary_cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "    loss = binary_cross_entropy(labels,logits)\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "Cx1RCTGpKUph",
        "outputId": "1eedeb89-6bf8-4aaf-b74c-d85921438adc"
      },
      "source": [
        "u = tf.Variable(tf.random.uniform(shape=[num_words,hidd_unit],minval=-0.1,maxval=0.1,dtype=tf.dtypes.float32))   \n",
        "w = tf.Variable(tf.random.uniform(shape=[hidd_unit,hidd_unit],minval=-0.1,maxval=0.1,dtype=tf.dtypes.float32))\n",
        "b = tf.Variable(tf.random.uniform(shape=[hidd_unit],minval=-0.1,maxval=0.1,dtype=tf.dtypes.float32))\n",
        "v = tf.Variable(tf.random.uniform(shape=[hidd_unit,1],minval=-0.1,maxval=0.1,dtype=tf.dtypes.float32))\n",
        "c = tf.Variable(tf.random.uniform(shape=[1],minval=-0.1,maxval=0.1,dtype=tf.dtypes.float32))\n",
        "\n",
        "start_var = tf.Variable(tf.zeros([hidd_unit,hidd_unit])) # initial state started with zero tensors\n",
        "\n",
        "start_time = time.time()\n",
        "train_loop()\n",
        "end_time = time.time()\n",
        "print(\"Total Time taken: \",end_time-start_time)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Av_Loss: 0.6979770064353943 Av_Accuracy: 0.487199991941452\n",
            "Av_Loss: 0.6987341046333313 Av_Accuracy: 0.49079999327659607\n",
            "Av_Loss: 0.6989322304725647 Av_Accuracy: 0.49373334646224976\n",
            "Av_Loss: 0.6981040239334106 Av_Accuracy: 0.49470001459121704\n",
            "Av_Loss: 0.6968080401420593 Av_Accuracy: 0.49535998702049255\n",
            "Av_Loss: 0.6953018307685852 Av_Accuracy: 0.49779999256134033\n",
            "Av_Loss: 0.6941268444061279 Av_Accuracy: 0.49714285135269165\n",
            "Av_Loss: 0.6933719515800476 Av_Accuracy: 0.49845001101493835\n",
            "Av_Loss: 0.6922779083251953 Av_Accuracy: 0.49862220883369446\n",
            "Av_Loss: 0.6912077069282532 Av_Accuracy: 0.5\n",
            "Total Time taken:  6043.511757612228\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIhP_OE-KUph"
      },
      "source": [
        "# Food for thought #1 and Food for thought #6\n",
        "\n",
        "-Why is this wasteful? Can you think of a smarter padding scheme that is more efficient? Consider the fact that RNNs can work on arbitrary sequence lengths, and that training mini batches are pretty much independent of each other.\n",
        "\n",
        "-pad_sequences allows for pre or post padding. Try both to see the difference. Which option do you think is better? Recall that we use the final time step output from our model.\n",
        "\n",
        "\n",
        "\n",
        "> Model : We have implemented a Recurrent Neural Network(RNN) with recurrent connections between hidden units, that reads an entire sequence and then produces a single output.\n",
        "\n",
        "\n",
        "\n",
        "> Data Observation: The length of the input sequences vary from 11 to 2494.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> Case 1: If we pad all the sequences to the maximum length which is 2494(post), considering the implementation, the model exhausts most of its gradient on the padded garbage sequences and doesn't give raise to any useful features pertaining to the task.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> Case 2: If we pad all the sequences to the maximum length which is 2494(Pre), although we couldn't evidently observe the difference between post and pre padding techniques(thanks to colab for disconnecting us nth time :/), when we logically analysed, we concluded that \"pre\" padding could be more advantageous compared to \"post\" as we are extracting the output from the final layers which is the output(good/garbage) is a result of actual data and the gradient is consumed by layers which have input sequences rather than some random padding value.\n",
        "\n",
        "\n",
        "\n",
        "> Case 3: Truncating and Padding to some fixed sequence length say 200 (which is what we did in this week's experiments), would have similar effect as case 1 and 2, apart from that since we are truncating we could loose some important features which would have helped the model learn better. \n",
        "\n",
        "\n",
        "\n",
        "> Case 4: So we thought how about may be we could  make minibatches, of varying lengths, for all the sequences of lengths 1-100 pad in such a way they are all of the length 100, 101-200 to 200 and so on, we couldn't get this idea working, but may be this could be better than cases 1-3.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "> Case 5: Another idea we got was how about we don't pad at all and since RNNs can work with varied sequence length, we came across \"tf.ragged\", which helps create datasets of variable lengths, this seems to be the best in terms of performance, at least for our model. \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yj2VP5IbKzIM"
      },
      "source": [
        "# Food for thought 2: \n",
        "Between truncating long sequences and removing them, which option do you think is better? Why? \n",
        "\n",
        "\n",
        "\n",
        "> Truncating is the better option because, in this way we are not loosing the training data. In this case, we are just trying to predict the sentiment behind a review and most of the time the first few words of the review would carry features which would help with better learning. Hence using truncated sentences would serve us better.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDV0KYkELMcD"
      },
      "source": [
        "# Food for thought #3: \n",
        "\n",
        "Can you think of a way to avoid the one-hot vectors completely? Even if you cannot implement it, a conceptual idea is fine.\n",
        "\n",
        "\n",
        "\n",
        "> Although, one hot encoding is a widely accepted and used way of representing textual data and they are the best when we are dealing with tf-idf or n-grams, they come with a disadvantage of being very sparse. One of the solutions to this is Word2Vec(skip gram/CBOW).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPT3V8d1LT5u"
      },
      "source": [
        "# Food for thought #4: \n",
        "\n",
        "How can it be that we can choose how many outputs we have, i.e. how can both be correct? Are there differences between both choices as well as (dis)advantages relative to each other?\n",
        "\n",
        "\n",
        "\n",
        "> For an arbitrary number of classes, normally a softmax layer is appended to the model so the outputs would have probabilistic properties by design. Sigmoid is the same as softmax as it could be thought as having two outputs, but one of them has all weights equal to zero and therefore its output will be always equal to zero. Hence, choosing 1 output in contrast to 2 outputs for binary classifiers is better since fewer parameters and computation are needed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EHh6k5XVLf5Q"
      },
      "source": [
        "# Food for thought #7 : \n",
        "\n",
        "Can you think of a way to prevent the RNN from computing new states on padded time steps? One idea might be to “pass through” the previous state in case the current time step is padding. Note that, within a batch, some sequences might be padded for a given time step while others are not.\n",
        "\n",
        "\n",
        "\n",
        "> Solution 1 : We could maybe use no padding at all, i.e., using tf.ragged.\n",
        "\n",
        "> Solution 2 : (Just a vague idea) Along with storing the data and labels we could also store the length of the sequences and when the RNN reaches the last time step we could stop the hidden layer calculation and send the hidden layer output to output layer (thanks to RNN's flexibility, this is feasible)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WD849HYENPYM"
      },
      "source": [
        "References:\n",
        "\n",
        "\n",
        "\n",
        "1.   Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. Deep learning. MIT press, 2016.\n",
        "2.   https://ovgu-ailab.github.io/idl2021/ass5.html\n",
        "3.   https://towardsdatascience.com/using-tensorflow-ragged-tensors-2af07849a7bd\n",
        "\n",
        "\n"
      ]
    }
  ]
}