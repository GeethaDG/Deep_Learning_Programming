{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Assignment 7: Attention-based Neural Machine Translation**\n",
    "By Geetha Doddapaneni Gopinath (229498), Sri Chandana Hudukula Ram Kumar (231616),Joy Rakshit(231681)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "y4LNmaPeTV15"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import typing\n",
    "from typing import Any, Tuple\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_text as tf_text\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import pathlib\n",
    "from google.colab import drive\n",
    "import os "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "kwgPzJpsgMb2"
   },
   "outputs": [],
   "source": [
    "os.chdir('/content/drive/MyDrive/Colab Notebooks/')\n",
    "path_to_file = pathlib.Path(\"/content/drive/MyDrive/Colab Notebooks/deu-eng/deu.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6hw-6AnccmA1",
    "outputId": "59b6015a-8e74-4ee7-b479-607ff747f7b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d_D0ixRSS_RM"
   },
   "source": [
    "# Classes and/or  Functions Needed for the Assignment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9aydaWyg12P"
   },
   "source": [
    "## Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "YFAoFkdyg2N4"
   },
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "  text = path.read_text(encoding='utf-8')\n",
    "\n",
    "  lines = text.splitlines()\n",
    "  pairs = [line.split('\\t')[:-1] for line in lines]\n",
    "\n",
    "  inp = [inp for targ, inp in pairs]\n",
    "  targ = [targ for targ, inp in pairs]\n",
    "\n",
    "  return targ, inp\n",
    "\n",
    "def data_replace(data):\n",
    "  for i in range(len(data)):\n",
    "    temp = list(data[i])\n",
    "    for j in range(len(temp)):\n",
    "      if temp[j]=='ß':\n",
    "        temp[j] = 'ss'\n",
    "    data[i]=\"\".join(temp)\n",
    "  return data\n",
    "\n",
    "#Text Preprocessing function (Unicode normalization to split accented characters and replace compatibility characters with their ASCII equivalents)\n",
    "def tf_lower_and_split_punct(text):\n",
    "  # Split accecented characters.\n",
    "  text = tf_text.normalize_utf8(text, 'NFKD')\n",
    "  text = tf.strings.lower(text)\n",
    "  # Keep space, a to z, and select punctuation.\n",
    "  text = tf.strings.regex_replace(text, '[^ a-z.?!,¿]', '')\n",
    "  # Add spaces around punctuation.\n",
    "  text = tf.strings.regex_replace(text, '[.?!,¿]', r' \\0 ')\n",
    "  # Strip whitespace.\n",
    "  text = tf.strings.strip(text)\n",
    "\n",
    "  text = tf.strings.join(['[START]', text, '[END]'], separator=' ')\n",
    "  return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FzpVhG01R82l"
   },
   "source": [
    "## Shape Checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "cm55yMxvR1Tq"
   },
   "outputs": [],
   "source": [
    "class ShapeChecker():\n",
    "  def __init__(self):\n",
    "    # Keep a cache of every axis-name seen\n",
    "    self.shapes = {}\n",
    "\n",
    "  def __call__(self, tensor, names, broadcast=False):\n",
    "    if not tf.executing_eagerly():\n",
    "      return\n",
    "\n",
    "    if isinstance(names, str):\n",
    "      names = (names,)\n",
    "\n",
    "    shape = tf.shape(tensor)\n",
    "    rank = tf.rank(tensor)\n",
    "    print(rank)\n",
    "    print(shape)\n",
    "    if rank != len(names):\n",
    "      raise ValueError(f'Rank mismatch:\\n'\n",
    "                       f'    found {rank}: {shape.numpy()}\\n'\n",
    "                       f'    expected {len(names)}: {names}\\n')\n",
    "\n",
    "    for i, name in enumerate(names):\n",
    "      if isinstance(name, int):\n",
    "        old_dim = name\n",
    "      else:\n",
    "        old_dim = self.shapes.get(name, None)\n",
    "      new_dim = shape[i]\n",
    "\n",
    "      if (broadcast and new_dim == 1):\n",
    "        continue\n",
    "\n",
    "      if old_dim is None:\n",
    "        # If the axis name is new, add its length to the cache.\n",
    "        self.shapes[name] = new_dim\n",
    "        continue\n",
    "\n",
    "      if new_dim != old_dim:\n",
    "        raise ValueError(f\"Shape mismatch for dimension: '{name}'\\n\"\n",
    "                         f\"    found: {new_dim}\\n\"\n",
    "                         f\"    expected: {old_dim}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ism_qA9lSXiF"
   },
   "source": [
    "## Encoder\n",
    "\n",
    "The encoder:\n",
    "\n",
    "- Takes a list of token IDs (from input_text_processor).\n",
    "- Looks up an embedding vector for each token (Using a layers.Embedding).\n",
    "- Processes the embeddings into a new sequence (Using a layers.GRU).\n",
    "- Returns:\n",
    "  - The processed sequence. This will be passed to the attention head.\n",
    "  - The internal state. This will be used to initialize the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "NxZoF3CpSeFF"
   },
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, input_vocab_size, embedding_dim, enc_units):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.enc_units = enc_units\n",
    "    self.input_vocab_size = input_vocab_size\n",
    "\n",
    "    # The embedding layer converts tokens to vectors\n",
    "    self.embedding = tf.keras.layers.Embedding(self.input_vocab_size,\n",
    "                                               embedding_dim)\n",
    "\n",
    "\n",
    "    # The GRU RNN layer processes those vectors sequentially.\n",
    "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
    "                                   # Return the sequence and state\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "  def call(self, tokens, state=None):\n",
    "    shape_checker = ShapeChecker()\n",
    "    shape_checker(tokens, ('batch', 's'))\n",
    "\n",
    "    # 2. The embedding layer looks up the embedding for each token.\n",
    "    vectors = self.embedding(tokens)\n",
    "    shape_checker(vectors, ('batch', 's', 'embed_dim'))\n",
    "\n",
    "    # 3. The GRU processes the embedding sequence.\n",
    "    #    output shape: (batch, s, enc_units)\n",
    "    #    state shape: (batch, enc_units)\n",
    "    output, state = self.gru(vectors, initial_state=state)\n",
    "    shape_checker(output, ('batch', 's', 'enc_units'))\n",
    "    shape_checker(state, ('batch', 'enc_units'))\n",
    "\n",
    "    # 4. Returns the new sequence and its state.\n",
    "    return output, state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJxDaRgFSpxD"
   },
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "_gGa_KqgSi-9"
   },
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "  def __init__(self, units):\n",
    "    super().__init__()\n",
    "    # For Eqn. (4), the  Bahdanau attention\n",
    "    self.W1 = tf.keras.layers.Dense(units, use_bias=False)\n",
    "    self.W2 = tf.keras.layers.Dense(units, use_bias=False)\n",
    "\n",
    "    self.attention = tf.keras.layers.AdditiveAttention()\n",
    "\n",
    "  def call(self, query, value, mask):\n",
    "    shape_checker = ShapeChecker()\n",
    "    shape_checker(query, ('batch', 't', 'query_units'))\n",
    "    shape_checker(value, ('batch', 's', 'value_units'))\n",
    "    shape_checker(mask, ('batch', 's'))\n",
    "\n",
    "    # From Eqn. (4), `W1@ht`.\n",
    "    w1_query = self.W1(query)\n",
    "    shape_checker(w1_query, ('batch', 't', 'attn_units'))\n",
    "\n",
    "    # From Eqn. (4), `W2@hs`.\n",
    "    w2_key = self.W2(value)\n",
    "    shape_checker(w2_key, ('batch', 's', 'attn_units'))\n",
    "\n",
    "    query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n",
    "    value_mask = mask\n",
    "\n",
    "    context_vector, attention_weights = self.attention(\n",
    "        inputs = [w1_query, value, w2_key],\n",
    "        mask=[query_mask, value_mask],\n",
    "        return_attention_scores = True,\n",
    "    )\n",
    "    shape_checker(context_vector, ('batch', 't', 'value_units'))\n",
    "    shape_checker(attention_weights, ('batch', 't', 's'))\n",
    "\n",
    "    return context_vector, attention_weights\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ctX5o7nStff"
   },
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "YzAjhqSXTsjw"
   },
   "outputs": [],
   "source": [
    "import typing\n",
    "from typing import Any, Tuple\n",
    "\n",
    "class DecoderInput(typing.NamedTuple):\n",
    "  new_tokens: Any\n",
    "  enc_output: Any\n",
    "  mask: Any\n",
    "\n",
    "class DecoderOutput(typing.NamedTuple):\n",
    "  logits: Any\n",
    "  attention_weights: Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "yICpNthVSwKL"
   },
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, output_vocab_size, embedding_dim, dec_units):\n",
    "    super(Decoder, self).__init__()\n",
    "    self.dec_units = dec_units\n",
    "    self.output_vocab_size = output_vocab_size\n",
    "    self.embedding_dim = embedding_dim\n",
    "\n",
    "    # For Step 1. The embedding layer convets token IDs to vectors\n",
    "    self.embedding = tf.keras.layers.Embedding(self.output_vocab_size,\n",
    "                                               embedding_dim)\n",
    "\n",
    "    # For Step 2. The RNN keeps track of what's been generated so far.\n",
    "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    # For step 3. The RNN output will be the query for the attention layer.\n",
    "    self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    # For step 4. Eqn. (3): converting `ct` to `at`\n",
    "    self.Wc = tf.keras.layers.Dense(dec_units, activation=tf.math.tanh,\n",
    "                                    use_bias=False)\n",
    "\n",
    "    # For step 5. This fully connected layer produces the logits for each\n",
    "    # output token.\n",
    "    self.fc = tf.keras.layers.Dense(self.output_vocab_size)\n",
    "\n",
    "  def call(self,inputs: DecoderInput,state=None) -> Tuple[DecoderOutput, tf.Tensor]:\n",
    "        shape_checker = ShapeChecker()\n",
    "        shape_checker(inputs.new_tokens, ('batch', 't'))\n",
    "        shape_checker(inputs.enc_output, ('batch', 's', 'enc_units'))\n",
    "        shape_checker(inputs.mask, ('batch', 's'))\n",
    "\n",
    "        if state is not None:\n",
    "            shape_checker(state, ('batch', 'dec_units'))\n",
    "\n",
    "        # Step 1. Lookup the embeddings\n",
    "        vectors = self.embedding(inputs.new_tokens)\n",
    "        shape_checker(vectors, ('batch', 't', 'embedding_dim'))\n",
    "\n",
    "        # Step 2. Process one step with the RNN\n",
    "        rnn_output, state = self.gru(vectors, initial_state=state)\n",
    "\n",
    "        shape_checker(rnn_output, ('batch', 't', 'dec_units'))\n",
    "        shape_checker(state, ('batch', 'dec_units'))\n",
    "\n",
    "        # Step 3. Use the RNN output as the query for the attention over the\n",
    "        # encoder output.\n",
    "        context_vector, attention_weights = self.attention(\n",
    "            query=rnn_output, value=inputs.enc_output, mask=inputs.mask)\n",
    "        shape_checker(context_vector, ('batch', 't', 'dec_units'))\n",
    "        shape_checker(attention_weights, ('batch', 't', 's'))\n",
    "\n",
    "        # Step 4. Eqn. (3): Join the context_vector and rnn_output\n",
    "        #     [ct; ht] shape: (batch t, value_units + query_units)\n",
    "        context_and_rnn_output = tf.concat([context_vector, rnn_output], axis=-1)\n",
    "\n",
    "        # Step 4. Eqn. (3): `at = tanh(Wc@[ct; ht])`\n",
    "        attention_vector = self.Wc(context_and_rnn_output)\n",
    "        shape_checker(attention_vector, ('batch', 't', 'dec_units'))\n",
    "\n",
    "        # Step 5. Generate logit predictions:\n",
    "        logits = self.fc(attention_vector)\n",
    "        shape_checker(logits, ('batch', 't', 'output_vocab_size'))\n",
    "\n",
    "        return DecoderOutput(logits, attention_weights), state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EdnNJP-DUJbE"
   },
   "source": [
    "## Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "gs1cbjxJUNXs"
   },
   "outputs": [],
   "source": [
    "class MaskedLoss(tf.keras.losses.Loss):\n",
    "  def __init__(self):\n",
    "    self.name = 'masked_loss'\n",
    "    self.loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "        from_logits=True, reduction='none')\n",
    "\n",
    "  def __call__(self, y_true, y_pred):\n",
    "    shape_checker = ShapeChecker()\n",
    "    shape_checker(y_true, ('batch', 't'))\n",
    "    shape_checker(y_pred, ('batch', 't', 'logits'))\n",
    "\n",
    "    # Calculate the loss for each item in the batch.\n",
    "    loss = self.loss(y_true, y_pred)\n",
    "    shape_checker(loss, ('batch', 't'))\n",
    "\n",
    "    # Mask off the losses on padding.\n",
    "    mask = tf.cast(y_true != 0, tf.float32)\n",
    "    shape_checker(mask, ('batch', 't'))\n",
    "    loss *= mask\n",
    "\n",
    "    # Return the total.\n",
    "    return tf.reduce_sum(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PEEJ-dMyURd1"
   },
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "s7qNXj_QUbNn"
   },
   "outputs": [],
   "source": [
    "class TrainTranslator(tf.keras.Model):\n",
    "    def __init__(self, embedding_dim, units,input_text_processor,output_text_processor,use_tf_function=True):\n",
    "        super().__init__()\n",
    "        # Build the encoder and decoder\n",
    "        encoder = Encoder(input_text_processor.vocabulary_size(),embedding_dim, units)\n",
    "        decoder = Decoder(output_text_processor.vocabulary_size(),embedding_dim, units)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.input_text_processor = input_text_processor\n",
    "        self.output_text_processor = output_text_processor\n",
    "        self.use_tf_function = use_tf_function\n",
    "        self.shape_checker = ShapeChecker()\n",
    "\n",
    "    def train_step(self, inputs):\n",
    "        self.shape_checker = ShapeChecker()\n",
    "        if self.use_tf_function:\n",
    "            return self._tf_train_step(inputs)\n",
    "        else:\n",
    "            return self._train_step(inputs)\n",
    "\n",
    "    def _preprocess(self, input_text, target_text):\n",
    "        self.shape_checker(input_text, ('batch',))\n",
    "        self.shape_checker(target_text, ('batch',))\n",
    "\n",
    "        # Convert the text to token IDs\n",
    "        input_tokens = self.input_text_processor(input_text)\n",
    "        target_tokens = self.output_text_processor(target_text)\n",
    "        self.shape_checker(input_tokens, ('batch', 's'))\n",
    "        self.shape_checker(target_tokens, ('batch', 't'))\n",
    "\n",
    "        # Convert IDs to masks.\n",
    "        input_mask = input_tokens != 0\n",
    "        self.shape_checker(input_mask, ('batch', 's'))\n",
    "\n",
    "        target_mask = target_tokens != 0\n",
    "        self.shape_checker(target_mask, ('batch', 't'))\n",
    "\n",
    "        return input_tokens, input_mask, target_tokens, target_mask\n",
    "\n",
    "    def _train_step(self, inputs):\n",
    "        input_text, target_text = inputs\n",
    "\n",
    "        (input_tokens, input_mask,target_tokens, target_mask) = self._preprocess(input_text, target_text)\n",
    "\n",
    "        max_target_length = tf.shape(target_tokens)[1]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Encode the input\n",
    "            enc_output, enc_state = self.encoder(input_tokens)\n",
    "            self.shape_checker(enc_output, ('batch', 's', 'enc_units'))\n",
    "            self.shape_checker(enc_state, ('batch', 'enc_units'))\n",
    "\n",
    "            # Initialize the decoder's state to the encoder's final state.\n",
    "            # This only works if the encoder and decoder have the same number of\n",
    "            # units.\n",
    "            dec_state = enc_state\n",
    "            loss = tf.constant(0.0)\n",
    "\n",
    "            for t in tf.range(max_target_length - 1):\n",
    "                # Pass in two tokens from the target sequence:\n",
    "                # 1. The current input to the decoder.\n",
    "                # 2. The target for the decoder's next prediction.\n",
    "                new_tokens = target_tokens[:, t:t + 2]\n",
    "                step_loss, dec_state = self._loop_step(new_tokens, input_mask,\n",
    "                                                     enc_output, dec_state)\n",
    "                loss = loss + step_loss\n",
    "\n",
    "            # Average the loss over all non padding tokens.\n",
    "            average_loss = loss / tf.reduce_sum(tf.cast(target_mask, tf.float32))\n",
    "\n",
    "        # Apply an optimization step\n",
    "        variables = self.trainable_variables\n",
    "        gradients = tape.gradient(average_loss, variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {'batch_loss': average_loss}\n",
    "\n",
    "    def _loop_step(self, new_tokens, input_mask, enc_output, dec_state):\n",
    "        input_token, target_token = new_tokens[:, 0:1], new_tokens[:, 1:2]\n",
    "\n",
    "        # Run the decoder one step.\n",
    "        decoder_input = DecoderInput(new_tokens=input_token,\n",
    "                                     enc_output=enc_output,\n",
    "                                     mask=input_mask)\n",
    "        dec_result, dec_state = self.decoder(decoder_input, state=dec_state)\n",
    "        self.shape_checker(dec_result.logits, ('batch', 't1', 'logits'))\n",
    "        self.shape_checker(dec_result.attention_weights, ('batch', 't1', 's'))\n",
    "        self.shape_checker(dec_state, ('batch', 'dec_units'))\n",
    "\n",
    "        # `self.loss` returns the total for non-padded tokens\n",
    "        y = target_token\n",
    "        y_pred = dec_result.logits\n",
    "        step_loss = self.loss(y, y_pred)\n",
    "\n",
    "        return step_loss, dec_state\n",
    "\n",
    "    @tf.function(input_signature=[[tf.TensorSpec(dtype=tf.string, shape=[None]),\n",
    "                               tf.TensorSpec(dtype=tf.string, shape=[None])]])\n",
    "    def _tf_train_step(self, inputs):\n",
    "      return self._train_step(inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PpH6kaPBU1jk"
   },
   "source": [
    "## Batch Logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "iNOAz86xU3wG"
   },
   "outputs": [],
   "source": [
    "class BatchLogs(tf.keras.callbacks.Callback):\n",
    "  def __init__(self, key):\n",
    "    self.key = key\n",
    "    self.logs = []\n",
    "\n",
    "  def on_train_batch_end(self, n, logs):\n",
    "    self.logs.append(logs[self.key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uuUgxv5VLmND"
   },
   "source": [
    "## Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "DD3EBIpkLlpg"
   },
   "outputs": [],
   "source": [
    "class Translator(tf.Module):\n",
    "\n",
    "  def __init__(self, encoder, decoder, input_text_processor,\n",
    "               output_text_processor):\n",
    "    self.encoder = encoder\n",
    "    self.decoder = decoder\n",
    "    self.input_text_processor = input_text_processor\n",
    "    self.output_text_processor = output_text_processor\n",
    "\n",
    "    self.output_token_string_from_index = (\n",
    "        tf.keras.layers.StringLookup(\n",
    "            vocabulary=output_text_processor.get_vocabulary(),\n",
    "            mask_token='',\n",
    "            invert=True))\n",
    "\n",
    "    # The output should never generate padding, unknown, or start.\n",
    "    index_from_string = tf.keras.layers.StringLookup(\n",
    "        vocabulary=output_text_processor.get_vocabulary(), mask_token='')\n",
    "    token_mask_ids = index_from_string(['', '[UNK]', '[START]']).numpy()\n",
    "\n",
    "    token_mask = np.zeros([index_from_string.vocabulary_size()], dtype=np.bool)\n",
    "    token_mask[np.array(token_mask_ids)] = True\n",
    "    self.token_mask = token_mask\n",
    "\n",
    "    self.start_token = index_from_string(tf.constant('[START]'))\n",
    "    self.end_token = index_from_string(tf.constant('[END]'))\n",
    "\n",
    "  def tokens_to_text(self, result_tokens):\n",
    "    shape_checker = ShapeChecker()\n",
    "    shape_checker(result_tokens, ('batch', 't'))\n",
    "    result_text_tokens = self.output_token_string_from_index(result_tokens)\n",
    "    shape_checker(result_text_tokens, ('batch', 't'))\n",
    "\n",
    "    result_text = tf.strings.reduce_join(result_text_tokens,\n",
    "                                       axis=1, separator=' ')\n",
    "    shape_checker(result_text, ('batch'))\n",
    "\n",
    "    result_text = tf.strings.strip(result_text)\n",
    "    shape_checker(result_text, ('batch',))\n",
    "    return result_text\n",
    "\n",
    "  def sample(self, logits, temperature):\n",
    "    shape_checker = ShapeChecker()\n",
    "    # 't' is usually 1 here.\n",
    "    shape_checker(logits, ('batch', 't', 'vocab'))\n",
    "    shape_checker(self.token_mask, ('vocab',))\n",
    "\n",
    "    token_mask = self.token_mask[tf.newaxis, tf.newaxis, :]\n",
    "    shape_checker(token_mask, ('batch', 't', 'vocab'), broadcast=True)\n",
    "\n",
    "    # Set the logits for all masked tokens to -inf, so they are never chosen.\n",
    "    logits = tf.where(self.token_mask, -np.inf, logits)\n",
    "\n",
    "    if temperature == 0.0:\n",
    "      new_tokens = tf.argmax(logits, axis=-1)\n",
    "    else: \n",
    "      logits = tf.squeeze(logits, axis=1)\n",
    "      new_tokens = tf.random.categorical(logits/temperature,\n",
    "                                          num_samples=1)\n",
    "\n",
    "    shape_checker(new_tokens, ('batch', 't'))\n",
    "\n",
    "    return new_tokens\n",
    "  \n",
    "  def translate(self,\n",
    "                       input_text, *,\n",
    "                       max_length=50,\n",
    "                       return_attention=True,\n",
    "                       temperature=1.0):\n",
    "    batch_size = tf.shape(input_text)[0]\n",
    "    input_tokens = self.input_text_processor(input_text)\n",
    "    enc_output, enc_state = self.encoder(input_tokens)\n",
    "\n",
    "    dec_state = enc_state\n",
    "    new_tokens = tf.fill([batch_size, 1], self.start_token)\n",
    "\n",
    "    result_tokens = []\n",
    "    attention = []\n",
    "    done = tf.zeros([batch_size, 1], dtype=tf.bool)\n",
    "\n",
    "    for _ in range(max_length):\n",
    "      dec_input = DecoderInput(new_tokens=new_tokens,\n",
    "                              enc_output=enc_output,\n",
    "                              mask=(input_tokens!=0))\n",
    "\n",
    "      dec_result, dec_state = self.decoder(dec_input, state=dec_state)\n",
    "\n",
    "      attention.append(dec_result.attention_weights)\n",
    "\n",
    "      new_tokens = self.sample(dec_result.logits, temperature)\n",
    "\n",
    "      # If a sequence produces an `end_token`, set it `done`\n",
    "      done = done | (new_tokens == self.end_token)\n",
    "      # Once a sequence is done it only produces 0-padding.\n",
    "      new_tokens = tf.where(done, tf.constant(0, dtype=tf.int64), new_tokens)\n",
    "\n",
    "      # Collect the generated tokens\n",
    "      result_tokens.append(new_tokens)\n",
    "\n",
    "      if tf.executing_eagerly() and tf.reduce_all(done):\n",
    "        break\n",
    "\n",
    "    # Convert the list of generates token ids to a list of strings.\n",
    "    result_tokens = tf.concat(result_tokens, axis=-1)\n",
    "    result_text = self.tokens_to_text(result_tokens)\n",
    "\n",
    "    if return_attention:\n",
    "      attention_stack = tf.concat(attention, axis=1)\n",
    "      return {'text': result_text, 'attention': attention_stack}\n",
    "    else:\n",
    "      return {'text': result_text}\n",
    "\n",
    "  @tf.function(input_signature=[tf.TensorSpec(dtype=tf.string, shape=[None])])\n",
    "  def tf_translate(self, input_text):\n",
    "    return self.translate(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8pt8OeixT5fS"
   },
   "source": [
    "# Data and Training \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YA0l1YQ-jltU"
   },
   "source": [
    "### Download and prepare Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "BIlAN2oyUD4j"
   },
   "outputs": [],
   "source": [
    "targ, inp = load_data(path_to_file) #Note : Target-> English Translation  Input -> German Input \n",
    "inp = data_replace(inp) #Replacing 'ß' by 'ss'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "y101HHFUignT"
   },
   "outputs": [],
   "source": [
    "#Hyper Parameters \n",
    "BUFFER_SIZE = len(inp)\n",
    "BATCH_SIZE = 64\n",
    "max_vocab_size = 5000\n",
    "embedding_dim = 256\n",
    "units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "c2PuBq6MieTZ"
   },
   "outputs": [],
   "source": [
    "#Create a tf.data dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inp, targ)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MBeVHLkDioz7",
    "outputId": "bb1e1e01-bdad-4e0c-da42-7fc985c5e64b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'Tom weiss das jetzt.' b'Ich kann in 30 Minuten dort sein.'\n",
      " b'Verrichte einfach deine Arbeit!'\n",
      " b'\\xe2\\x80\\x9eHast du schon einmal original ungarisches Gulasch gegessen?\\xe2\\x80\\x9c\\xc2\\xa0\\xe2\\x80\\x93 \\xe2\\x80\\x9eNein, aber ich w\\xc3\\xbcrde wirklich gerne mal.\\xe2\\x80\\x9c'\n",
      " b'Ich m\\xc3\\xb6chte dir gerne dieses Bild von Tom zeigen.'], shape=(5,), dtype=string)\n",
      "\n",
      "tf.Tensor(\n",
      "[b'Tom knows that now.' b'I can be there in 30 minutes.'\n",
      " b'Just do your job.'\n",
      " b'\"Have you ever had genuine Hungarian goulash?\" \"No, but I\\'d really like to.\"'\n",
      " b\"I'd like to show this picture of Tom to you.\"], shape=(5,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "for example_input_batch, example_target_batch in dataset.take(1):\n",
    "  print(example_input_batch[:5])\n",
    "  print()\n",
    "  print(example_target_batch[:5])\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UyvBd3eLj9ag"
   },
   "source": [
    "### Text preprocessing\n",
    "\n",
    "- Standardization using the function 'tf_lower_and_split_punct', where Unicode normalization will be the first step\n",
    "- Text Vectorisation - wrapped up in a tf.keras.layers.TextVectorization layer which will handle the vocabulary extraction and conversion of input text to sequences of tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "bTF8yfIXjLdY"
   },
   "outputs": [],
   "source": [
    "#Text Vectorization\n",
    "#Returned tokens are zero padded.\n",
    "input_text_processor = tf.keras.layers.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct,\n",
    "    max_tokens=max_vocab_size)\n",
    "input_text_processor.adapt(inp)\n",
    "output_text_processor = tf.keras.layers.TextVectorization(\n",
    "    standardize=tf_lower_and_split_punct,\n",
    "    max_tokens=max_vocab_size)\n",
    "output_text_processor.adapt(targ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xxg0CfCjN5TT",
    "outputId": "130c6ec9-b420-4d70-c93b-060c6b4fd890"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "3880/3880 [==============================] - 1944s 498ms/step - batch_loss: 1.6297\n",
      "Epoch 2/5\n",
      "3880/3880 [==============================] - 1932s 498ms/step - batch_loss: 0.9116\n",
      "Epoch 3/5\n",
      "3880/3880 [==============================] - 1912s 493ms/step - batch_loss: 0.7225\n",
      "Epoch 4/5\n",
      "3880/3880 [==============================] - 1925s 496ms/step - batch_loss: 0.5865\n",
      "Epoch 5/5\n",
      "3880/3880 [==============================] - 1925s 496ms/step - batch_loss: 0.4813\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6c612db290>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_translator = TrainTranslator(\n",
    "    embedding_dim, units,\n",
    "    input_text_processor=input_text_processor,\n",
    "    output_text_processor=output_text_processor,\n",
    "    use_tf_function=True)\n",
    "\n",
    "# Configure the loss and optimizer\n",
    "train_translator.compile(\n",
    "    optimizer=tf.optimizers.Adam(),\n",
    "    loss=MaskedLoss(),\n",
    ")\n",
    "batch_loss = BatchLogs('batch_loss')\n",
    "train_translator.fit(dataset, epochs=5,\n",
    "                     callbacks=[batch_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HgQOSPlH-fnB",
    "outputId": "14276111-44e6-42f0-bff9-8718abb3223e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as encoder_layer_call_fn, encoder_layer_call_and_return_conditional_losses, decoder_layer_call_fn, decoder_layer_call_and_return_conditional_losses, embedding_layer_call_fn while saving (showing 5 of 60). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: translator/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: translator/assets\n"
     ]
    }
   ],
   "source": [
    "translator = Translator(\n",
    "    encoder=train_translator.encoder,\n",
    "    decoder=train_translator.decoder,\n",
    "    input_text_processor=input_text_processor,\n",
    "    output_text_processor=output_text_processor,\n",
    ")\n",
    "\n",
    "tf.saved_model.save(translator, 'translator',\n",
    "                    signatures={'serving_default': translator.tf_translate})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pnQcDxkIFtcm",
    "outputId": "93477aa9-4ee3-4e3a-a775-3680ad2b52d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "im going hunting .\n",
      "hi , its fine .\n",
      "theyre busy .\n"
     ]
    }
   ],
   "source": [
    "reloaded = tf.saved_model.load('translator')\n",
    "three_input_text = tf.constant([\n",
    "    'Ich reise nach Berlin.',\n",
    "    'Hallo!',\n",
    "    'Sie sind da',\n",
    "])\n",
    "result = reloaded.tf_translate(three_input_text)\n",
    "for tr in result['text']:\n",
    "  print(tr.numpy().decode())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4hV85HtnUG7i",
    "outputId": "8d6819e1-8883-466b-f979-a85cc4d0d469"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am traveling by air .\n",
      "hi .\n",
      "they are in touch heaven .\n"
     ]
    }
   ],
   "source": [
    "reloaded = tf.saved_model.load('translator')\n",
    "three_input_text = tf.constant([\n",
    "    'Ich reise nach Berlin.',\n",
    "    'Hallo!',\n",
    "    'Sie sind da',\n",
    "])\n",
    "result = reloaded.tf_translate(three_input_text)\n",
    "for tr in result['text']:\n",
    "  print(tr.numpy().decode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "BD-GblKPF_mV",
    "outputId": "e88884d8-773f-4bed-e1bc-09f94cf11bdf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'CE/token')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(batch_loss.logs)\n",
    "plt.ylim([0, 3])\n",
    "plt.xlabel('Batch #')\n",
    "plt.ylabel('CE/token')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "An1qkI0RpmLs"
   },
   "source": [
    "#### Questions\n",
    "\n",
    "1. Which parts of the sentence are used as a token? Each character, each word, or are some words split up?\n",
    "\n",
    "  Each word/punctuation(example: verb as well as verb+tense) are taken as tokens. Please see the output of the cells below for an illustration.\n",
    "\n",
    "2. Do the same tokens in different language have the same ID? e.g. Would the same token index map to the German word die and to the English word die?\n",
    "  \n",
    "  In the illustration below, we can see that even though the words 'zoo' and 'bitter' have the same meaning in both language, we can see that they have different token ids, hence two words even if they have the same spelling, there is no guarantee that they will have the same ID. It would be just a coincidence if this happens.\n",
    "\n",
    "3. What is the relation between the encoder output and the encoder hidden state which is used to initialize the decoder hidden state (for the architecture used in the tutorial)?\n",
    "\n",
    "  - Encoder output is the tensor for all timesteps, which has shape [batch, time, units]\n",
    "  - Encoder state is the output tensor for the last timestep, which has shape [batch, units]\n",
    "  - Encoder state or the internal state is used to initialize the decoder\n",
    "\n",
    "4.  Is the decoder attending to all previous positions, including the previous decoder predictions?\n",
    "  \n",
    "  Decoder at timestep 't' uses the state at 't-1' to attend to the entire encoder output to calculate the attention weights(followed by context vector and attention vector)\n",
    "\n",
    "5. Does the encoder output change in different decoding steps?\n",
    "   \n",
    "   No, encoder processes the whole sequence and generates an output, which remains constant and serves as value to the decoder. \n",
    "\n",
    "6. Does the context vector change in different decoding steps?\n",
    "\n",
    "   Yes, attention weights are calculated at every timestep of the decoder based on the previous timestep of the decoder, and using the updated attention weight, decoder attends to the encoder output to generate the context vector. Hence, cotext vector changes at every decoding step.\n",
    "\n",
    "7. The decoder uses teacher forcing. Does this mean the time steps can be computed in parallel?\n",
    "\n",
    "  In this implementation, the  context vector is generated based on previous state of the decoder, hence even though we are using teacher forcing (giving the previous token as input to predict the current token), we cannot parallelise the decoder \n",
    "\n",
    "8. Why is a mask applied to the loss function?\n",
    "   \n",
    "   Due to rectagular implementation .i.e, in a batch the records are padded to the length of the longest sequence, the mask is applied to ensure we are considering the loss pertaining only to the sequence rather than padding. \n",
    "\n",
    "9. When translating the same sentence multiple times, do you get the same result? Why (not)? If not, what changes need to be made to get the same result each time?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OwUVYWs_qgrx",
    "outputId": "68254e0c-8820-4f37-9483-b061a95ec9ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------\n",
      "  For Question 1 \n",
      "------------------\n",
      "Vocabulary extracted from Input sequences: \n",
      "['', '[UNK]', '[START]', '[END]', '.', 'tom', 'ich', 'das', ',', 'ist', '?', 'zu', 'nicht', 'in', 'hat', 'wir', 'du', 'die', 'den', 'sie']\n",
      "Vocabulary extracted from Target sequences: \n",
      "['', '[UNK]', '[START]', '[END]', '.', 'the', 'tom', 'to', 'i', 'a', 'this', 'my', '?', 'you', 'in', 'have', 'that', 'is', 'your', 'we']\n",
      "------------------\n",
      "  For Question 2 \n",
      "------------------\n",
      "\n",
      "Example 1: \n",
      "Input Token IDs:\n",
      " tf.Tensor([  2   6 250  26  60   4   3   0   0   0   0   0   0   0   0   0   0   0], shape=(18,), dtype=int64)\n",
      "Corresponding Text:\n",
      " [START] ich arbeite im zoo . [END]           \n",
      "Target Token IDs:\n",
      " tf.Tensor([  2   8  67 226   5  64   4   3   0   0   0   0   0   0   0], shape=(15,), dtype=int64)\n",
      "Corresponding Text:\n",
      " [START] i work at the zoo . [END]        \n",
      "\n",
      "\n",
      "Example 2: \n",
      "Input Token IDs:\n",
      " tf.Tensor([  2 227  96   9  11 235   4   3   0   0   0   0   0   0   0   0   0   0], shape=(18,), dtype=int64)\n",
      "Corresponding Text:\n",
      " [START] dieser tee ist zu bitter . [END]          \n",
      "Target Token IDs:\n",
      " tf.Tensor([  2  10  99  17  32 217   4   3   0   0   0   0   0   0   0], shape=(15,), dtype=int64)\n",
      "Corresponding Text:\n",
      " [START] this tea is too bitter . [END]        \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('------------------\\n  For Question 1 \\n------------------')\n",
    "input_text_processor.adapt(example_input_batch)\n",
    "output_text_processor.adapt(example_target_batch)\n",
    "print('Vocabulary extracted from Input sequences: \\n{}'.format(input_text_processor.get_vocabulary()[:20]))\n",
    "print('Vocabulary extracted from Target sequences: \\n{}'.format(output_text_processor.get_vocabulary()[:20]))\n",
    "print('------------------\\n  For Question 2 \\n------------------')\n",
    "example_input_tokens = input_text_processor(example_input_batch)\n",
    "print('\\nExample 1: \\nInput Token IDs:\\n',example_input_tokens[0])\n",
    "input_vocab = np.array(input_text_processor.get_vocabulary())\n",
    "tokens = input_vocab[example_input_tokens[0].numpy()]\n",
    "print('Corresponding Text:\\n {}'.format(' '.join(tokens)))\n",
    "\n",
    "example_target_tokens = output_text_processor(example_target_batch)\n",
    "print('Target Token IDs:\\n',example_target_tokens[0])\n",
    "target_vocab = np.array(output_text_processor.get_vocabulary())\n",
    "tokens = target_vocab[example_target_tokens[0].numpy()]\n",
    "print('Corresponding Text:\\n {} \\n'.format(' '.join(tokens)))\n",
    "\n",
    "print('\\nExample 2: \\nInput Token IDs:\\n',example_input_tokens[1])\n",
    "tokens = input_vocab[example_input_tokens[1].numpy()]\n",
    "print('Corresponding Text:\\n {}'.format(' '.join(tokens)))\n",
    "print('Target Token IDs:\\n',example_target_tokens[1])\n",
    "tokens = target_vocab[example_target_tokens[1].numpy()]\n",
    "print('Corresponding Text:\\n {} \\n'.format(' '.join(tokens)))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "FzpVhG01R82l",
    "ism_qA9lSXiF",
    "JJxDaRgFSpxD",
    "2ctX5o7nStff",
    "EdnNJP-DUJbE",
    "PpH6kaPBU1jk",
    "uuUgxv5VLmND"
   ],
   "name": "Assignment7_NMT.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
